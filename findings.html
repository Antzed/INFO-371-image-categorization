<!DOCTYPE html>
<html>
<head>
<title>findings.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="findings">findings</h1>
<p>By Anthony Zhang, William Wang,</p>
<h2 id="preparing-the-environment">preparing the environment</h2>
<p>In order to get the best result, We decided to run the network training locally on one of our laptops. The specs of the laptop is:</p>
<table>
<thead>
<tr>
<th>hardware type</th>
<th>hardware name</th>
</tr>
</thead>
<tbody>
<tr>
<td>CPU</td>
<td>11 gen i7-11800H @ 2.3GHz</td>
</tr>
<tr>
<td>GPU</td>
<td>Nvidia Geforce RTX 3070 laptop GPU</td>
</tr>
</tbody>
</table>
<p>We decided to use docker to create the environment. This is way i can skip the process of figuring out which different python packages we need install/missed.</p>
<p>We also allowed GPU processing, which will speed up the training process.</p>
<h2 id="preparing-the-data">Preparing the data</h2>
<p>to prepare the data, we first get extracted the data and put them in in here a file structure like this:</p>
<pre class="hljs"><code><div> PS7/
    └── data/
        ├── train
        └── validate
</div></code></pre>
<p>Then we run a few commands to get a sense of what the data look like. This is the number from both the training data and the validation data.</p>
<pre class="hljs"><code><div>find . -type f -name &quot;*_EN-*&quot; | wc -l
find . -type f -name &quot;*_DA-*&quot; | wc -l
find . -type f -name &quot;*_RU-*&quot; | wc -l
find . -type f -name &quot;*_ZN-*&quot; | wc -l
find . -type f -name &quot;*_TH-*&quot; | wc -l
</div></code></pre>
<p>Number of EN files: 10492 <br/>
Number of DA files: 5114<br/>
Number of RU files: 8204<br/>
Number of ZN files: 6702<br/>
Number of TH files: 9324<br/></p>
<p>We then took a subset of a 1000 EN files and 1000 ZN files and backed up the rest in <code>train_big</code> and <code>validation_big</code> folder under the same directory for later use.</p>
<pre class="hljs"><code><div>find . -<span class="hljs-built_in">type</span> f -name <span class="hljs-string">"*_EN-*"</span> | shuf -n 1000 | xargs -I {} cp {} ../train
find . -<span class="hljs-built_in">type</span> f -name <span class="hljs-string">"*_ZH-*"</span> | shuf -n 1000 | xargs -I {} cp {} ../train
</div></code></pre>
<p>Finally, we  applied the same logic to the validation dataset and copied 225 <code>EN</code> and 225 <code>ZN</code> images into the validation folder, along with 50 <code>DA</code> images. We choose the number 225, 225, and 50 because they add up to 500. We want to follow the 80:20 split rule. So when we have 2000 images as out training set, we will need 500 image for the validation set.</p>
<pre class="hljs"><code><div>find . -<span class="hljs-built_in">type</span> f -name <span class="hljs-string">"*_EN-*"</span> | shuf -n 225 | xargs -I {} cp {} ../validation
find . -<span class="hljs-built_in">type</span> f -name <span class="hljs-string">"*_ZN-*"</span> | shuf -n 225 | xargs -I {} cp {} ../validation
find . -<span class="hljs-built_in">type</span> f -name <span class="hljs-string">"*_DA-*"</span> | shuf -n 50 | xargs -I {} cp {} ../validation
</div></code></pre>
<h2 id="what-the-code-does">What the code does</h2>
<h3 id="intro">Intro</h3>
<p>For this project, we processed a project to construct, train, and assess a convolution neural network (CNN) model to categorize photographs according to the type of written text they include, which are DA: Danish, EN: English, RU: Russian, TH: Thai, ZN: Chinese. This project is a component of a larger challenge to use deep learning methods for picture identification, with a focus on language recognition of text images. To well understand the code we gave, I would like to separate them into a few parts to explain:</p>
<h3 id="creating-model">Creating Model:</h3>
<p>After downloading and testing the dataset, the first part we did was to create the model. We used a ‘Sequential’ model from Keras, which works well with a layer stack in which there is precisely one input tensor and one output tensor for each layer. In this model architecture, two layers played an important role.</p>
<p>To prevent overfitting, a 2D convolutional layer (Conv2D) with ReLU activation is followed by batch normalization, max pooling, and dropout. It performs a convolution operation for the upcoming data working. This entails calculating the dot product between the filter weights and the input pixels each filter covers at each point when it is slid across the input image (both horizontally and vertically). In our project, we can change the number of parameters by changing the number of filters (or kernels) used in the convolution operation and the kernel_size (the size of the filter matrix). Also, we can change the clarity of the image by changing the stride size. When we increased the stride size, it would essentially reduce the resolution of the input text image in the feature map output. Conversely, when we decrease the stride size, it expands the feature map's spatial dimensions upon output. This makes it possible to analyze the image in greater depth and capture the image's finer details.</p>
<p>The two ‘Dense’ layers are also important for creating the model, the first ‘Dense’ layer is to interpret the feature. The ‘kernel_initializer = initializers.HeNormal()’ argument specifies the initialization method for the weights. Also, when we change the units of the Dense, it would impact the capacity and performance of the neural network. For example, if we reduce the number of Denser units, it will help mitigate overfitting when the model is complex. The second ‘Dense’ layer is for classification. It intended to assign each image to a specific linguistic category. This layer has the same number of neurons as the number of language categories we are attempting to predict.</p>
<h3 id="training-and-validation-data-generator--model-training">Training and validation data generator &amp; Model Training:</h3>
<p>For this part, we used the ‘ImageDataGenerator’ function, which is a technique to increase the diversity of the training dataset without explicitly increasing the number of images. One important finding for this part is to change the size of the epoch, which would impact both performance metrics and the risk of overfitting or underfitting. When we increase the size of the epoch, it will lead to higher accuracy on the training dataset. However, it will also increase the possibility of overfitting. When we decrease the size of the epoch, it will increase the possibility of underfitting, but it will shorten the training time, which will make the process more efficient. The findings below clearly show this.</p>
<h3 id="validation-data-preparation--categorical-prediction">Validation data preparation &amp; Categorical prediction:</h3>
<p>For this part, we made a validation process to categorize data for different languages and made categorical predictions. Filenames and the associated categories are used to build a data frame called ‘validationResults’. The existence of particular substrings (EN for English, DA for Danish, and others designated as Unknown) in the filenames determines the category. After that, we used the ‘ImageDataGenerator’ function again to create an instance for the validation data with only ‘rescale=1./255’ applied to normalize the image pixel values. After that, we add the predicted category indices to the data frame and get the predicted probability array shape. In comparison with the actual categories, we got the confusion matrix and validation accuracy at the end of this part. Also, we can get the wrong results in validation data.</p>
<h3 id="visualization">Visualization:</h3>
<p>As the last part of our project, to visualize the comparisons we have derived, we have chosen to present the four correct outputs and four wrong outputs to show the differences, which helps point out any persistent biases or flaws in the model and in directing future enhancements to the training procedure, data preparation, or model design.</p>
<h2 id="getting-the-code-to-work">Getting the code to work</h2>
<p>Based on the code we got in last week's lab, we researched and adjusted it. In comparison with the <code>square-circle-cross</code> code, the language data have more complex categories and relatively clear output. In that case, we adjusted it, and the script will map the training data based on the language:</p>
<pre class="hljs"><code><div>trainingResults = pd.DataFrame({
    <span class="hljs-string">'filename'</span>: filenames,
    <span class="hljs-string">'category'</span>: np.where(pd.Series(filenames).str.contains(<span class="hljs-string">'EN'</span>), <span class="hljs-string">'EN'</span>,
                         np.where(pd.Series(filenames).str.contains(<span class="hljs-string">'ZN'</span>), <span class="hljs-string">'ZN'</span>, <span class="hljs-string">'Unknown'</span>))
})
</div></code></pre>
<p>We also applied the same logic in the later validation dataframe:</p>
<pre class="hljs"><code><div>validationResults = pd.DataFrame({
    <span class="hljs-string">'filename'</span>: fNames,
    <span class="hljs-string">'category'</span>: np.where(pd.Series(fNames).str.contains(<span class="hljs-string">'EN'</span>), <span class="hljs-string">'EN'</span>,
                np.where(pd.Series(fNames).str.contains(<span class="hljs-string">'ZN'</span>), <span class="hljs-string">'ZN'</span>,
                np.where(pd.Series(fNames).str.contains(<span class="hljs-string">'DA'</span>), <span class="hljs-string">'DA'</span>, <span class="hljs-string">'Unknown'</span>)))
})
</div></code></pre>
<p>Finally, we changed the code defining image properties:</p>
<pre class="hljs"><code><div>targetWidth, targetHeight, channels = <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>
</div></code></pre>
<p>Afterward the code runs. From the 1000 EN and ZN images, we get a validation accuracy of 0.89. This is with a network of (64, 128, 128, 128), kernel size 3, stride 1, and 10 epochs.</p>
<p>the confusion matrix:</p>
<table>
<thead>
<tr>
<th>predicted category</th>
<th>EN</th>
<th>ZN</th>
</tr>
</thead>
<tbody>
<tr>
<td>DA</td>
<td>49</td>
<td>1</td>
</tr>
<tr>
<td>EN</td>
<td>221</td>
<td>4</td>
</tr>
<tr>
<td>ZN</td>
<td>1</td>
<td>224</td>
</tr>
</tbody>
</table>
<h2 id="test-language">Test language</h2>
<h3 id="attempt-1">attempt 1</h3>
<p>Now let play with this network.</p>
<p>The first thing that needs to be done is increase the sample size. Hence we will use 5000 image from each languages(EN, ZN) for training and 1000 images for each language for testing.</p>
<p>So for the first attempt, we have a neural network of layer (64, 128, 128, 128), which in total uses 63072386 parameters and we trained the network for 10 epochs.</p>
<p>For this attempt we get a validation accuracy of 0.9995. however, it took about 43 minutes to run, which is pretty long training time.</p>
<p>Here is the confusion matrix for this attempt:</p>
<table>
<thead>
<tr>
<th>predicted category</th>
<th>EN</th>
<th>ZN</th>
</tr>
</thead>
<tbody>
<tr>
<td>EN</td>
<td>1000</td>
<td>0</td>
</tr>
<tr>
<td>ZN</td>
<td>1</td>
<td>999</td>
</tr>
</tbody>
</table>
<p>we can see that we identified one chinese image wrong. If we print it out, we can see that it is because that particular image do not have many feature points to begin with. Most of the space in the image is empty. The chinese characters shown on there has been cut in half.</p>
<p><img src="./output/classification_results_attempt1.png" alt="attempt1">
The first image on the top is the wrongly predicted images.</p>
<p>So overall, this is a pretty accurate model. However, the the down side is that the training time is too long.</p>
<h3 id="attempt-2">attempt 2</h3>
<p>So for my second attempt, I want reduce the training time by reducing the layers from 4 to 2, which means we'll get rid of one Conv2d layer and one dense layer.</p>
<p>Surprisingly, doing this have increased my number of parameters from the previous 63072386 to 132130562. On a closer look however, this makes a lot of sense. By getting rid of the Conv2D layer, we also got rid of the pooling layer, which was responsible for reduces the number of parameters. Thus, the total number of parameters increases when such parameters mask the flattening layer.</p>
<p>The second attempt took about 8.6 minutes and resulted in a validation accuracy of 0.999. The result confusion matrix is the following:</p>
<table>
<thead>
<tr>
<th>predicted category</th>
<th>EN</th>
<th>ZN</th>
</tr>
</thead>
<tbody>
<tr>
<td>EN</td>
<td>998</td>
<td>2</td>
</tr>
<tr>
<td>ZN</td>
<td>0</td>
<td>1000</td>
</tr>
</tbody>
</table>
<p>We can see that the frequency of wrong predict increased by 1, which is not so much given that we have cut down the training time by roughly 3/4. Here some demo on the image prediction:</p>
<p><img src="./output/classification_results_attempt2.png" alt="attempt2"></p>
<p>Additionally, as the top two images demonstrate, we are still confronted with the same issue as the last attempt: a large amount of white space in the image appears to reduce the model's effectiveness. This issue appears to be caused by the fact that most images contain very little white space, which means the model doesn't have a lot of data to train on.</p>
<h3 id="attempt-3">attempt 3</h3>
<p>So in order to fix this problem, we will try and use the original training data set with all the EN images and ZN images. But in order to maintain our training time, we will increase the strides to 2.</p>
<p>So this leave us with 8442 EN and 5396 ZN training images and a total parameter of 32515842.</p>
<p>The result produced a validation accuracy is 0.985 with a training time of roughly 10.5 minutes. and the confusion matrix is this:</p>
<table>
<thead>
<tr>
<th>predicted category</th>
<th>EN</th>
<th>ZN</th>
</tr>
</thead>
<tbody>
<tr>
<td>EN</td>
<td>2001</td>
<td>49</td>
</tr>
<tr>
<td>ZN</td>
<td>0</td>
<td>1306</td>
</tr>
</tbody>
</table>
<p><img src="./output/classification_results_attempt3.png" alt="attempt3">.</p>
<p>This shows that it was not an equal trade between the increase in training sample and increase in strides. The quality of the model decreased with the increase of strides from 1 to 2.</p>
<h2 id="adding-more-language">Adding more language.</h2>
<h3 id="attempt-4">attempt 4</h3>
<p>So for attempt 4, we added one language - Thai data to train our model. And we yielded 63% validation accuracy for three languages.</p>
<p>Here is the confusion matrix:</p>
<table>
<thead>
<tr>
<th>matrix predicted category</th>
<th>EN</th>
<th>TH</th>
<th>ZN</th>
</tr>
</thead>
<tbody>
<tr>
<td>EN</td>
<td>2037</td>
<td>6</td>
<td>7</td>
</tr>
<tr>
<td>TH</td>
<td>395</td>
<td>9</td>
<td>1495</td>
</tr>
<tr>
<td>ZN</td>
<td>1</td>
<td>0</td>
<td>1305</td>
</tr>
</tbody>
</table>
<p>Therefore, we found that as the complexity of the data increased from 2 types to 3 types, the accuracy took hit when the networks remain to be the same.</p>
<p><img src="./output/classification_results_attempt4.png" alt="attempt4"></p>
<h3 id="attempt-5">attempt 5</h3>
<p>In order to increasing the accuracy of the model, we attempted to add a dense layer with 128 nodes. Nonetheless, we discovered that the validation accuracy stays at roughly 63.8%. We were perplexed as to why a dense layer cannot be added. Overfitting or declining returns was our conjecture.</p>
<p>Here is the confusion matrix:</p>
<table>
<thead>
<tr>
<th>predicted category</th>
<th>EN</th>
<th>TH</th>
<th>ZN</th>
</tr>
</thead>
<tbody>
<tr>
<td>EN</td>
<td>2049</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>TH</td>
<td>1830</td>
<td>69</td>
<td>0</td>
</tr>
<tr>
<td>ZN</td>
<td>3</td>
<td>1303</td>
<td>0</td>
</tr>
</tbody>
</table>
<h3 id="attempt-6">attempt 6</h3>
<p>To keep increasing the accuracy of the model, we added other convolution layer with 128 nodes and MaxPooling2D. This does increased the validation accuracy to 73.7% and decreased our parameter.</p>
<table>
<thead>
<tr>
<th>predicted category</th>
<th>EN</th>
<th>TH</th>
<th>ZN</th>
</tr>
</thead>
<tbody>
<tr>
<td>EN</td>
<td>2049</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>TH</td>
<td>3</td>
<td>524</td>
<td>1372</td>
</tr>
<tr>
<td>ZN</td>
<td>7</td>
<td>0</td>
<td>1299</td>
</tr>
</tbody>
</table>
<h3 id="attempt-7">attempt 7</h3>
<p>In order to keep simplifying the model, we would like to keep reducing the number of parameters. As a result, we decided to remove one Dense layer and replace it by a Average Pooling Layer.</p>
<p>The number of parameters decrease significantly from 63073411 to 15821444. However, the training time shot up to a staggering 1 hour and 40 minutes, and validation accuracy was only at 0.68, which is quite low.</p>
<table>
<thead>
<tr>
<th>matrix predicted category</th>
<th>EN</th>
<th>TH</th>
<th>ZN</th>
</tr>
</thead>
<tbody>
<tr>
<td>EN</td>
<td>2033</td>
<td>5</td>
<td>12</td>
</tr>
<tr>
<td>TH</td>
<td>1574</td>
<td>241</td>
<td>84</td>
</tr>
<tr>
<td>ZN</td>
<td>3</td>
<td>21</td>
<td>1282</td>
</tr>
</tbody>
</table>
<h2 id="en-vs-da">EN vs DA</h2>
<p>Now we want to focus on making the model differentiate between EN and DA</p>
<h3 id="attempt-8">attempt 8</h3>
<p>Since we have the GPU, we decided to just continue with using all of the EN and DA images in the data set.</p>
<p>We choose to start with identifying EN and DA images after learning previously that utilizing only two hidden layers might get a relatively respectable validation score.</p>
<p>So with this attempt, we ran the training for about 13 minutes, which is relatively fast.</p>
<p>And we get the validation score of 0.84. This was lower when we tried the same method on classifying EN image to ZN.</p>
<p>The confusion matrix look like this:</p>
<p>confusion matrix (validation)</p>
<table>
<thead>
<tr>
<th>matrix predicted category</th>
<th>DA</th>
<th>EN</th>
</tr>
</thead>
<tbody>
<tr>
<td>DA</td>
<td>808</td>
<td>211</td>
</tr>
<tr>
<td>EN</td>
<td>274</td>
<td>1776</td>
</tr>
</tbody>
</table>
<h3 id="attempt-9">attempt 9</h3>
<p>Finally, we tried adding one more Conv2d Layer since previous attempt shows that adding both the Conv2d layer and dense layer result in super long training time. We also increased the stride to 2 for the same reasons</p>
<p>We finished the training in 12 minute got the validation score of 0.65. This was a little big surprising because we thought the added complexity should have increased the validation accuracy. This shows that the increase in stride have more influence in the validation accuracy than added layers. By adding this layer, it facilitates the network's ability to more effectively gather the necessary characteristics required to discern between English and Danish in the text pictures.</p>
<table>
<thead>
<tr>
<th>predicted category</th>
<th>DA</th>
<th>EN</th>
</tr>
</thead>
<tbody>
<tr>
<td>DA</td>
<td>1019</td>
<td>0</td>
</tr>
<tr>
<td>EN</td>
<td>1064</td>
<td>986</td>
</tr>
</tbody>
</table>
<p>From the graphing we can see that a lot of English images was recognized  as Danish. There were no distinct characteristic of those wrongly categorized images compare to those who is correctly categorized. This once gain prove the difficulty of separating English and Danish.</p>
<p><img src="./output/classification_results_attempt9.png" alt="attempt9"></p>
<h2 id="conclusion">Conclusion</h2>
<p>So overall, we can see that separating EN and ZN images are fairy easy. This is mostly likely because Chinese is a hieroglyphic language and all of its characters resemble the shape of a square. This square-like feature distinguish its elves from language like English, which relies entirely on the 26 alphabetical letters. The highest validation accuracy we have reached is 0.999 with two hidden layers (1 Conv2D and 1 dense layer) in around 8 to 9 minutes of training time with 10 epochs.</p>
<p>EN, ZN and TH are harder to distinguish compare to the just EN and ZN. This is partially due to the added complexity of trying to distinguish 3 language instead of 2, and because compare to the distinct contrast between EN and ZN, EN and TH is harder to distinguish. Although from the human perspective TH is obviously different from EN, their letter like symbols is similar to that of English in the network's perspective.
For this set of data we reached maximum of 0.737 using 4 hidden layers: two Conv2D　layer, two dense layer. The training time for this around 24 minutes, which is an obvious increase from before.</p>
<p>Finally, EN and DA is also very hard to distinguish. This is due to the fact that Danish and English essentially utilize the same alphabet. With two hidden layers—one Conv2d and one Dense—we were able to achieve a maximum validation accuracy of 0.84 in about 13 minutes. After that, we increased the strides to two and the hidden layers to three, which led to a lower accuracy of 0.65.  This implied that the number of layers may not have as much of an impact on validation accuracy as expected.</p>

</body>
</html>
